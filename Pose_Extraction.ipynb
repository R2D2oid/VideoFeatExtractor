{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Extractor\n",
    "\n",
    "This notebook extracts estimated pose using a pre-trained OpenPose[1,2] model. \n",
    "\n",
    " 1. Extract frames for a given video (fps)\n",
    " 2. Collect pose keypoints for all frames\n",
    "\n",
    "#### References:\n",
    "\n",
    "[1] *OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields*, Cao et al.\n",
    "\n",
    "[2] https://github.com/CMU-Perceptual-Computing-Lab/openpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_dir = '/usr/local/data02/zahra/datasets/Tempuckey/videos'\n",
    "frames_dir = '/usr/local/data02/zahra/datasets/Tempuckey/frames'\n",
    "op_output_dir = '/usr/local/data02/zahra/datasets/Tempuckey/feats/openpose'\n",
    "\n",
    "root_dir = '/usr/local/data01/zahra/repos/VideoFeatExtratotor'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract frames from a given video (fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sys_utils import *\n",
    "from utils.video_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_vids = get_subfolders_path(frames_dir)\n",
    "completed_vids = set([v.split('/')[-1] for v in completed_vids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = get_files_path(videos_dir)\n",
    "\n",
    "# extract frames\n",
    "for v in videos:\n",
    "    vname = v.replace('/','.').split('.')[-2]\n",
    "    \n",
    "    if vname in completed_vids:\n",
    "        print('{} was previously completed\\n'.format(vname))\n",
    "        continue\n",
    "        \n",
    "    f = '{}/{}'.format(frames_dir, vname)\n",
    "    create_folder(f)\n",
    "    \n",
    "    vc = cv2.VideoCapture(v)\n",
    "    count = get_video_frames(vc, f)\n",
    "    print('stored {} frames at {}\\n'.format(count, f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collect keypoints for all the frames in a clip \n",
    "### repeats for all clips and store heatmaps, keypoints, and ouput imgs for all frames in each clip\n",
    "\n",
    "- ### <span style=\"color:green\"> **2.1**</span> obtain joint heatmaps \n",
    "  - returns $H_j^t[x,y]$\n",
    "  - in our BODY\\_25 model, each image 1,2,...,25 contains all the same joints detected for everyone in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: openpose python api\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openpose import pyopenpose as op\n",
    "# setup the open pose model\n",
    "params = dict()\n",
    "params[\"model_folder\"] = root_dir + \"/models/\"\n",
    "params[\"heatmaps_add_parts\"] = True\n",
    "params[\"heatmaps_add_bkg\"] = False\n",
    "params[\"heatmaps_add_PAFs\"] = False\n",
    "\n",
    "# Starting OpenPose\n",
    "opWrapper = op.WrapperPython()\n",
    "opWrapper.configure(params)\n",
    "opWrapper.start()\n",
    "\n",
    "datum = op.Datum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_op_output(poseHeatMaps_t, outputImages_t, keypoints_t, beg_frame_idx, end_frame_idx, vname, op_output_dir):\n",
    "    \n",
    "    beg_frame_idx = str(beg_frame_idx).zfill(5)\n",
    "    end_frame_idx = str(end_frame_idx).zfill(5)\n",
    "    \n",
    "    # num_frames, num_keypoints/num_joints , img_width, img_height => dims = (T, 25, w, h)\n",
    "    # i.e. for image frame 0 we have 25 heatmaps (one per keypoint or joint)\n",
    "    output_path = '{}/{}_heatmaps_T_25_w_h_{}_{}'.format(op_output_dir, vname, beg_frame_idx, end_frame_idx)\n",
    "\n",
    "    print('{}'.format(output_path))\n",
    "    np.save(output_path , np.array(poseHeatMaps_t))\n",
    "\n",
    "    # outputImages_t[0].shape => (720, 1280, 3) \n",
    "    # one RGB image with dims = (W,H,3) per frame. and a total of T frames\n",
    "    output_path = '{}/{}_posed_imgs_T_w_h_3_{}_{}'.format(op_output_dir, vname, beg_frame_idx, end_frame_idx)\n",
    "\n",
    "    print('{}'.format(output_path))\n",
    "    np.save(output_path , np.array(outputImages_t))\n",
    "\n",
    "    # keypoints_t dims = (T, num_persons, num_keypoints, 3) => where 3 is [x,y,confidence] for each detected keypoint on a person\n",
    "    output_path = '{}/{}_keypoints_T_persons_25_3_{}_{}'.format(op_output_dir, vname, beg_frame_idx, end_frame_idx)\n",
    "\n",
    "    print('{}'.format(output_path))\n",
    "    np.save(output_path , np.array(keypoints_t))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get path to all the videos\n",
    "videos = get_files_path(videos_dir)\n",
    "\n",
    "## get list of test set video for which we need to compute openpose estimations\n",
    "labels_dir = '/usr/local/data02/zahra/datasets/Tempuckey/labels/tempuckey_video_info_gt_labels_split.csv'\n",
    "labels_df = pd.read_csv(labels_dir)\n",
    "\n",
    "test_set_video_names = list(labels_df[labels_df['split'] == 'test'].video_name)\n",
    "test_set_video_names = set([v.split('.')[0] for v in test_set_video_names])\n",
    "\n",
    "## get list of already completed (i.e. with extracted openpose)\n",
    "completed_video_names = get_files_path(op_output_dir)\n",
    "completed_video_names = set(['_'.join(v.split('/')[-1].split('_')[:3]) for v in completed_video_names])\n",
    "\n",
    "## pick the subset of videos that belong to test set and were not previously completed\n",
    "videos_to_process = []\n",
    "for v in videos:\n",
    "    vname = v.split('/')[-1].split('.')[0]\n",
    "    if vname in test_set_video_names and vname not in completed_video_names:\n",
    "        videos_to_process.append(v)\n",
    "        \n",
    "# videos = videos_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if videos_to_process == []:\n",
    "    print('openpose for all test videos was extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos = get_files_path(videos_dir)\n",
    "\n",
    "win = 320 # pack 320 frames into one npy file \n",
    "\n",
    "# extract pose heatmap and keypoints for each frame\n",
    "for v in videos:\n",
    "    vname = v.replace('/','.').split('.')[-2]\n",
    "    \n",
    "    if vname in completed_vids:\n",
    "        print('{} was previously completed'.format(vname))\n",
    "        continue\n",
    "        \n",
    "    f = '{}/{}'.format(frames_dir, vname)\n",
    "    frames = get_files_path(f)\n",
    "    \n",
    "    poseHeatMaps_t = []\n",
    "    outputImages_t = []\n",
    "    keypoints_t = []\n",
    "\n",
    "    # framewise pose, heatmap, keypoint extraction\n",
    "    for i in range(len(frames)):\n",
    "        img = cv2.imread(frames[i])\n",
    "\n",
    "        datum.cvInputData = img\n",
    "        opWrapper.emplaceAndPop([datum])\n",
    "\n",
    "        poseHeatMaps_t.append(datum.poseHeatMaps.copy())\n",
    "        outputImages_t.append(datum.cvOutputData.copy())\n",
    "        keypoints_t.append(datum.poseKeypoints.copy())\n",
    "        \n",
    "        if (i+1)%win == 0:\n",
    "            \n",
    "            beg_frame_idx = i - (win-1)\n",
    "            end_frame_idx = i\n",
    "\n",
    "            store_op_output(poseHeatMaps_t, outputImages_t, keypoints_t, beg_frame_idx, end_frame_idx, vname, op_output_dir)\n",
    "            \n",
    "            poseHeatMaps_t = []\n",
    "            outputImages_t = []\n",
    "            keypoints_t = []\n",
    "            \n",
    "    if i > end_frame_idx:\n",
    "        \n",
    "        beg_frame_idx = end_frame_idx\n",
    "        end_frame_idx = i\n",
    "\n",
    "        store_op_output(poseHeatMaps_t, outputImages_t, keypoints_t, beg_frame_idx, end_frame_idx, vname, op_output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('datum.poseHeatMaps.shape: \\t', datum.poseHeatMaps.shape) #25_body_joints\n",
    "\n",
    "# heat maps storing format: body parts + background + PAFs\n",
    "print('datum.poseKeypoints.shape: \\t', datum.poseKeypoints.shape) # (num_persons, num_joints_25, (x,y,confidence) )\n",
    "\n",
    "print('datum.cvInputData.shape: \\t', datum.cvInputData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
